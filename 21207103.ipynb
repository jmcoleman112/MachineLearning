{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=IMPORT PACKAGES=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "1df14f8471147ca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:30:52.713269Z",
     "start_time": "2025-10-24T21:30:49.454540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.base import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "\n"
   ],
   "id": "18a6854dabc57226",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=FILE DOWNLOAD=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "9930cab4247d2f22"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-24T20:51:45.172564Z",
     "start_time": "2025-10-24T20:43:23.931183Z"
    }
   },
   "source": [
    "#Initilizes Files and Header Information\n",
    "file_list  = [\n",
    "    \"SiCo30_01.txt\", \"GaCo11_01.txt\", \"SiCo27_01.txt\", \"SiCo14_01.txt\", \"SiCo17_01.txt\", \"GaCo02_01.txt\", \"GaCo13_01.txt\", \"GaCo17_01.txt\", \"GaCo08_01.txt\", \"SiCo18_01.txt\",\n",
    "    \"SiCo06_01.txt\", \"GaCo12_01.txt\", \"SiCo03_01.txt\", \"GaCo06_01.txt\", \"SiCo19_01.txt\", \"GaCo04_01.txt\", \"GaCo22_01.txt\", \"SiCo04_01.txt\", \"SiCo25_01.txt\", \"GaCo16_01.txt\",\n",
    "    \"SiCo10_01.txt\", \"SiCo20_01.txt\", \"SiCo21_01.txt\", \"GaCo09_01.txt\", \"SiCo13_01.txt\", \"SiCo09_01.txt\", \"GaCo01_01.txt\", \"GaCo05_01.txt\", \"SiCo26_01.txt\", \"GaCo10_01.txt\",\n",
    "    \"SiCo12_01.txt\", \"SiCo29_01.txt\", \"SiCo15_01.txt\", \"SiCo11_01.txt\", \"SiCo28_01.txt\", \"SiCo07_01.txt\", \"SiCo24_01.txt\", \"SiCo01_01.txt\", \"GaCo03_01.txt\", \"GaCo15_01.txt\",\n",
    "    \"SiPt34_01.txt\", \"GaPt13_01.txt\", \"GaPt14_01.txt\", \"SiPt13_01.txt\", \"SiPt38_01.txt\", \"SiPt21_01.txt\", \"SiPt02_01.txt\", \"SiPt07_01.txt\", \"GaPt03_01.txt\", \"GaPt30_01.txt\",\n",
    "    \"SiPt36_01.txt\", \"SiPt28_01.txt\", \"GaPt07_01.txt\", \"GaPt19_01.txt\", \"SiPt04_01.txt\", \"SiPt18_01.txt\", \"GaPt23_01.txt\", \"GaPt15_01.txt\", \"GaPt20_01.txt\", \"GaPt12_01.txt\",\n",
    "    \"SiPt39_01.txt\", \"SiPt32_01.txt\", \"SiPt30_01.txt\", \"GaPt22_01.txt\", \"SiPt40_01.txt\", \"GaPt05_01.txt\", \"GaPt33_01.txt\", \"SiPt33_01.txt\", \"SiPt10_01.txt\", \"GaPt08_01.txt\",\n",
    "    \"GaPt24_01.txt\", \"SiPt05_01.txt\", \"GaPt04_01.txt\", \"SiPt24_01.txt\", \"SiPt35_01.txt\", \"SiPt23_01.txt\", \"SiPt08_01.txt\", \"SiPt25_01.txt\", \"SiPt12_01.txt\", \"GaPt27_01.txt\"\n",
    "]\n",
    "cols = [\n",
    "    \"time\", \"sensor_l1\", \"sensor_l2\", \"sensor_l3\", \"sensor_l4\", \"sensor_l5\",\n",
    "    \"sensor_l6\", \"sensor_l7\", \"sensor_l8\", \"sensor_r1\", \"sensor_r2\", \"sensor_r3\",\n",
    "    \"sensor_r4\", \"sensor_r5\", \"sensor_r6\", \"sensor_r7\", \"sensor_r8\", \"total_left\", \"total_right\"\n",
    "]\n",
    "# Gets a List of all Patient Names by Cleaning Up file Names\n",
    "patients = [os.path.splitext(f)[0].replace(\"_01\", \"\") for f in file_list]\n",
    "\n",
    "\n",
    "base_url = \"https://physionet.org/files/gaitpdb/1.0.0/\"\n",
    "dst_dir = r\"Data/RawData/\"\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "# Code to Download files\n",
    "for fname in file_list:\n",
    "    url = base_url + fname+ \"?download\"\n",
    "    dst_path = os.path.join(dst_dir, fname)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(dst_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {fname} from {url} to {dst_path}\")\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Failed to download {fname}: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded SiCo30_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo30_01.txt?download to Data/RawData/SiCo30_01.txt\n",
      "Downloaded GaCo11_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo11_01.txt?download to Data/RawData/GaCo11_01.txt\n",
      "Downloaded SiCo27_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo27_01.txt?download to Data/RawData/SiCo27_01.txt\n",
      "Downloaded SiCo14_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo14_01.txt?download to Data/RawData/SiCo14_01.txt\n",
      "Downloaded SiCo17_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo17_01.txt?download to Data/RawData/SiCo17_01.txt\n",
      "Downloaded GaCo02_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo02_01.txt?download to Data/RawData/GaCo02_01.txt\n",
      "Downloaded GaCo13_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo13_01.txt?download to Data/RawData/GaCo13_01.txt\n",
      "Downloaded GaCo17_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo17_01.txt?download to Data/RawData/GaCo17_01.txt\n",
      "Downloaded GaCo08_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo08_01.txt?download to Data/RawData/GaCo08_01.txt\n",
      "Downloaded SiCo18_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo18_01.txt?download to Data/RawData/SiCo18_01.txt\n",
      "Downloaded SiCo06_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo06_01.txt?download to Data/RawData/SiCo06_01.txt\n",
      "Downloaded GaCo12_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo12_01.txt?download to Data/RawData/GaCo12_01.txt\n",
      "Downloaded SiCo03_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo03_01.txt?download to Data/RawData/SiCo03_01.txt\n",
      "Downloaded GaCo06_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo06_01.txt?download to Data/RawData/GaCo06_01.txt\n",
      "Downloaded SiCo19_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo19_01.txt?download to Data/RawData/SiCo19_01.txt\n",
      "Downloaded GaCo04_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo04_01.txt?download to Data/RawData/GaCo04_01.txt\n",
      "Downloaded GaCo22_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo22_01.txt?download to Data/RawData/GaCo22_01.txt\n",
      "Downloaded SiCo04_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo04_01.txt?download to Data/RawData/SiCo04_01.txt\n",
      "Downloaded SiCo25_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo25_01.txt?download to Data/RawData/SiCo25_01.txt\n",
      "Downloaded GaCo16_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo16_01.txt?download to Data/RawData/GaCo16_01.txt\n",
      "Downloaded SiCo10_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo10_01.txt?download to Data/RawData/SiCo10_01.txt\n",
      "Downloaded SiCo20_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo20_01.txt?download to Data/RawData/SiCo20_01.txt\n",
      "Downloaded SiCo21_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo21_01.txt?download to Data/RawData/SiCo21_01.txt\n",
      "Downloaded GaCo09_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo09_01.txt?download to Data/RawData/GaCo09_01.txt\n",
      "Downloaded SiCo13_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo13_01.txt?download to Data/RawData/SiCo13_01.txt\n",
      "Downloaded SiCo09_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo09_01.txt?download to Data/RawData/SiCo09_01.txt\n",
      "Downloaded GaCo01_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo01_01.txt?download to Data/RawData/GaCo01_01.txt\n",
      "Downloaded GaCo05_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo05_01.txt?download to Data/RawData/GaCo05_01.txt\n",
      "Downloaded SiCo26_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo26_01.txt?download to Data/RawData/SiCo26_01.txt\n",
      "Downloaded GaCo10_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo10_01.txt?download to Data/RawData/GaCo10_01.txt\n",
      "Downloaded SiCo12_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo12_01.txt?download to Data/RawData/SiCo12_01.txt\n",
      "Downloaded SiCo29_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo29_01.txt?download to Data/RawData/SiCo29_01.txt\n",
      "Downloaded SiCo15_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo15_01.txt?download to Data/RawData/SiCo15_01.txt\n",
      "Downloaded SiCo11_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo11_01.txt?download to Data/RawData/SiCo11_01.txt\n",
      "Downloaded SiCo28_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo28_01.txt?download to Data/RawData/SiCo28_01.txt\n",
      "Downloaded SiCo07_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo07_01.txt?download to Data/RawData/SiCo07_01.txt\n",
      "Downloaded SiCo24_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo24_01.txt?download to Data/RawData/SiCo24_01.txt\n",
      "Downloaded SiCo01_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiCo01_01.txt?download to Data/RawData/SiCo01_01.txt\n",
      "Downloaded GaCo03_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo03_01.txt?download to Data/RawData/GaCo03_01.txt\n",
      "Downloaded GaCo15_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaCo15_01.txt?download to Data/RawData/GaCo15_01.txt\n",
      "Downloaded SiPt34_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt34_01.txt?download to Data/RawData/SiPt34_01.txt\n",
      "Downloaded GaPt13_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt13_01.txt?download to Data/RawData/GaPt13_01.txt\n",
      "Downloaded GaPt14_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt14_01.txt?download to Data/RawData/GaPt14_01.txt\n",
      "Downloaded SiPt13_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt13_01.txt?download to Data/RawData/SiPt13_01.txt\n",
      "Downloaded SiPt38_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt38_01.txt?download to Data/RawData/SiPt38_01.txt\n",
      "Downloaded SiPt21_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt21_01.txt?download to Data/RawData/SiPt21_01.txt\n",
      "Downloaded SiPt02_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt02_01.txt?download to Data/RawData/SiPt02_01.txt\n",
      "Downloaded SiPt07_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt07_01.txt?download to Data/RawData/SiPt07_01.txt\n",
      "Downloaded GaPt03_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt03_01.txt?download to Data/RawData/GaPt03_01.txt\n",
      "Downloaded GaPt30_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt30_01.txt?download to Data/RawData/GaPt30_01.txt\n",
      "Downloaded SiPt36_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt36_01.txt?download to Data/RawData/SiPt36_01.txt\n",
      "Downloaded SiPt28_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt28_01.txt?download to Data/RawData/SiPt28_01.txt\n",
      "Downloaded GaPt07_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt07_01.txt?download to Data/RawData/GaPt07_01.txt\n",
      "Downloaded GaPt19_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt19_01.txt?download to Data/RawData/GaPt19_01.txt\n",
      "Downloaded SiPt04_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt04_01.txt?download to Data/RawData/SiPt04_01.txt\n",
      "Downloaded SiPt18_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt18_01.txt?download to Data/RawData/SiPt18_01.txt\n",
      "Downloaded GaPt23_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt23_01.txt?download to Data/RawData/GaPt23_01.txt\n",
      "Downloaded GaPt15_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt15_01.txt?download to Data/RawData/GaPt15_01.txt\n",
      "Downloaded GaPt20_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt20_01.txt?download to Data/RawData/GaPt20_01.txt\n",
      "Downloaded GaPt12_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt12_01.txt?download to Data/RawData/GaPt12_01.txt\n",
      "Downloaded SiPt39_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt39_01.txt?download to Data/RawData/SiPt39_01.txt\n",
      "Downloaded SiPt32_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt32_01.txt?download to Data/RawData/SiPt32_01.txt\n",
      "Downloaded SiPt30_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt30_01.txt?download to Data/RawData/SiPt30_01.txt\n",
      "Downloaded GaPt22_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt22_01.txt?download to Data/RawData/GaPt22_01.txt\n",
      "Downloaded SiPt40_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt40_01.txt?download to Data/RawData/SiPt40_01.txt\n",
      "Downloaded GaPt05_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt05_01.txt?download to Data/RawData/GaPt05_01.txt\n",
      "Downloaded GaPt33_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt33_01.txt?download to Data/RawData/GaPt33_01.txt\n",
      "Downloaded SiPt33_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt33_01.txt?download to Data/RawData/SiPt33_01.txt\n",
      "Downloaded SiPt10_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt10_01.txt?download to Data/RawData/SiPt10_01.txt\n",
      "Downloaded GaPt08_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt08_01.txt?download to Data/RawData/GaPt08_01.txt\n",
      "Downloaded GaPt24_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt24_01.txt?download to Data/RawData/GaPt24_01.txt\n",
      "Downloaded SiPt05_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt05_01.txt?download to Data/RawData/SiPt05_01.txt\n",
      "Downloaded GaPt04_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt04_01.txt?download to Data/RawData/GaPt04_01.txt\n",
      "Downloaded SiPt24_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt24_01.txt?download to Data/RawData/SiPt24_01.txt\n",
      "Downloaded SiPt35_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt35_01.txt?download to Data/RawData/SiPt35_01.txt\n",
      "Downloaded SiPt23_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt23_01.txt?download to Data/RawData/SiPt23_01.txt\n",
      "Downloaded SiPt08_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt08_01.txt?download to Data/RawData/SiPt08_01.txt\n",
      "Downloaded SiPt25_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt25_01.txt?download to Data/RawData/SiPt25_01.txt\n",
      "Downloaded SiPt12_01.txt from https://physionet.org/files/gaitpdb/1.0.0/SiPt12_01.txt?download to Data/RawData/SiPt12_01.txt\n",
      "Downloaded GaPt27_01.txt from https://physionet.org/files/gaitpdb/1.0.0/GaPt27_01.txt?download to Data/RawData/GaPt27_01.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=DATA STORING IN DATABASE=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "8f83d005b9bf75e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T10:20:53.819623Z",
     "start_time": "2025-10-25T10:20:50.236108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creates a SQLite Database so that data can be tracked maintained and manipulated slightly easier then in Massive Arrays\n",
    "db_file = \"Gait_Database.sqlite\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cur = conn.cursor()\n",
    "\n",
    "#Reads Demographic Data Into Database\n",
    "df = pd.read_excel(\"./Data/demographics.xls\" )\n",
    "first_col = df.columns[0]\n",
    "df = df[df[first_col].isin(patients)]\n",
    "df.to_sql(\"demographics\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Reads all patient data in to Database\n",
    "for i, file in enumerate(files):\n",
    "    df = pd.read_csv(f\"./Data/RawData/{file}\", sep=\"\\t\", header=None)\n",
    "    df.columns = headers\n",
    "    df.to_sql(patients[i], conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Dont actually need the time data, so we can remove this\n",
    "cols.pop(0)\n"
   ],
   "id": "feec5e47b7a9cf06",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=DATA IMPUTATION=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "9289d512da656042"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T21:37:41.149033Z",
     "start_time": "2025-10-24T21:37:41.146096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SQL command that fills in any \"NaN\" with the average based on Gender\n",
    "for col in [\"Height (meters)\", \"Weight (kg)\", \"Speed_01 (m/sec)\", \"TUAG\"]:\n",
    "    cur.execute(f\"\"\"\n",
    "        UPDATE demographics AS d\n",
    "        SET \"{col}\" = (\n",
    "            SELECT ROUND(AVG(d2.\"{col}\"), 2)\n",
    "            FROM demographics AS d2\n",
    "            WHERE (LOWER(d2.Gender)='male') = (LOWER(d.Gender)='male')\n",
    "        )\n",
    "        WHERE \"{col}\" IS NULL;\n",
    "    \"\"\")\n",
    "    print(f\"Filled missing values for {col}\")\n",
    "\n",
    "#Commit Change to Database\n",
    "conn.commit()"
   ],
   "id": "c4e6896ccbfd522b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values for Height (meters)\n",
      "Filled missing values for Weight (kg)\n",
      "Filled missing values for Speed_01 (m/sec)\n",
      "Filled missing values for TUAG\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=Feature Extraction=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "577cb4032808095d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T10:16:02.469212Z",
     "start_time": "2025-10-25T10:16:00.015358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fs = 100  # Hz\n",
    "\n",
    "#Method that will print a summary of the Features (Mean, min, max, std, etc)\n",
    "#Prints in LaTeX table form so I can copy it into report directly if I make any changes\n",
    "#Is a function because I need to do this twice- once before scaling, and once after\n",
    "def print_feature_stats(features, feature_names):\n",
    "    print(\"\\n=== Feature Summary Statistics ===\")\n",
    "    print(\"Feature & Min & Max & Mean & Std & Median \\\\\\\\\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    for name, col in zip(feature_names, features.T):\n",
    "        print(f\"{name} & {np.min(col):.2f} & {np.max(col):.2f} & \"\n",
    "              f\"{np.mean(col):.2f} & {np.std(col, ddof=1):.2f} & {np.median(col):.2f}\\\\\\\\\")\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "\n",
    "#Representations of Left and Right Toes/Heels\n",
    "L_HEEL, L_TOE = [\"sensor_l1\",\"sensor_l2\",\"sensor_l3\"], [\"sensor_l6\",\"sensor_l7\",\"sensor_l8\"]\n",
    "R_HEEL, R_TOE = [\"sensor_r1\",\"sensor_r2\",\"sensor_r3\"], [\"sensor_r6\",\"sensor_r7\",\"sensor_r8\"]\n",
    "\n",
    "#Initilize Features and Classiciations\n",
    "features = np.zeros((len(patients), 15), float)\n",
    "classification = np.zeros(len(patients), int)\n",
    "\n",
    "#For Each Patient:\n",
    "for i, pid in enumerate(patients):\n",
    "\n",
    "    #get their data and turn it into a dict called S\n",
    "    cur.execute(f'SELECT {\", \".join(cols)} FROM \"{pid}\"')\n",
    "    data = np.asarray(cur.fetchall(), float)\n",
    "    S = {c: data[:, j] for j, c in enumerate(cols)}\n",
    "    gender, age, speed, height, TUAG = cur.execute(\n",
    "    'SELECT Gender, Age, \"Speed_01 (m/sec)\", \"Height (meters)\", \"TUAG\" FROM demographics WHERE ID=?',\n",
    "    (pid,)\n",
    "    ).fetchone()\n",
    "\n",
    "    #Step Calculations\n",
    "    total_left, total_right = S[\"total_left\"], S[\"total_right\"]\n",
    "    steps_left,  _ = find_peaks(total_left,  prominence=400, height=0.5*np.max(total_left))\n",
    "    steps_right, _ = find_peaks(total_right, prominence=400, height=0.5*np.max(total_right))\n",
    "\n",
    "    #Stride Calculation\n",
    "    stride_left, stride_right = np.diff(steps_left)/fs, np.diff(steps_right)/fs\n",
    "    mean_stride_L, mean_stride_R = np.mean(stride_left), np.mean(stride_right)\n",
    "    stride_all = np.concatenate([stride_left, stride_right])\n",
    "\n",
    "    #Stance Calculation\n",
    "    stance_L, stance_R = (heel_L + toe_L) > 0, (heel_R + toe_R) > 0\n",
    "    stance_pct_L = np.array([100*np.mean(stance_L[a:b]) for a,b in zip(steps_left[:-1], steps_left[1:])])\n",
    "    stance_pct_R = np.array([100*np.mean(stance_R[a:b]) for a,b in zip(steps_right[:-1], steps_right[1:])])\n",
    "    mean_stance_L, mean_stance_R = np.mean(stance_pct_L), np.mean(stance_pct_R)\n",
    "\n",
    "    #Swing Calculation\n",
    "    swing_pct_L, swing_pct_R  = 100 - stance_pct_L, 100 - stance_pct_R\n",
    "    mean_swing_L, mean_swing_R = np.mean(swing_pct_L), np.mean(swing_pct_R)\n",
    "    swing_all  = np.concatenate([swing_pct_L, swing_pct_R])\n",
    "\n",
    "    #Step Calculation\n",
    "    idx_R_after_L = np.searchsorted(steps_right, steps_left)\n",
    "    valid_LR = idx_R_after_L < len(steps_right)\n",
    "    step_LR = (steps_right[idx_R_after_L[valid_LR]] - steps_left[valid_LR]) / fs\n",
    "    idx_L_after_R = np.searchsorted(steps_left, steps_right)\n",
    "    valid_RL = idx_L_after_R < len(steps_left)\n",
    "    step_RL = (steps_left[idx_L_after_R[valid_RL]] - steps_right[valid_RL]) / fs\n",
    "    mean_step_LR, mean_step_RL = np.mean(step_LR), np.mean(step_RL)\n",
    "    step_all   = np.concatenate([step_LR, step_RL])\n",
    "\n",
    "    #Heel-Toe Calculation\n",
    "    heel_L = np.mean(np.vstack([S[k] for k in L_HEEL]), axis=0)\n",
    "    toe_L  = np.mean(np.vstack([S[k] for k in L_TOE ]), axis=0)\n",
    "    heel_R = np.mean(np.vstack([S[k] for k in R_HEEL]), axis=0)\n",
    "    toe_R  = np.mean(np.vstack([S[k] for k in R_TOE ]), axis=0)\n",
    "    htr_L = np.mean(heel_L[stance_L]) / np.mean(toe_L[stance_L])\n",
    "    htr_R = np.mean(heel_R[stance_R]) / np.mean(toe_R[stance_R])\n",
    "\n",
    "    #Double Support Calculation\n",
    "    thrL, thrR = 0.02*np.max(total_left), 0.02*np.max(total_right)\n",
    "    contact_L, contact_R = total_left > thrL, total_right > thrR\n",
    "    n_min = min(contact_L.size, contact_R.size)\n",
    "    double_support = np.mean(contact_L[:n_min] & contact_R[:n_min])\n",
    "\n",
    "    features[i] = [\n",
    "        np.std(stride_all, ddof=1) / np.mean(stride_all),   # 0  Stride Time Var.\n",
    "        np.std(swing_all,  ddof=1) / np.mean(swing_all),    # 1  Swing Time Var.\n",
    "        np.std(step_all,   ddof=1) / np.mean(step_all),     # 2  Step Time Var.\n",
    "        mean_stride_L / mean_stride_R,                      # 3  Stride Time Asym.\n",
    "        mean_swing_L / mean_swing_R,                        # 4  Swing Time Asym.\n",
    "        mean_step_LR / mean_step_RL,                        # 5  Step Time Asym.\n",
    "        mean_stance_L / mean_stance_R,                      # 6  Stance Time Asym.\n",
    "        double_support,                                     # 7  Double Support\n",
    "        0.5*(htr_L + htr_R),                                # 8  Heel-Toe Pressure\n",
    "        htr_L / htr_R,                                      # 9  Heel-Toe Pressure Asym.\n",
    "        1.0 if gender.lower() == \"male\" else 0.0,           # 10 Gender\n",
    "        float(age),                                         # 11 Age\n",
    "        float(speed),                                       # 12 Speed\n",
    "        float(height),                                      # 13 Height\n",
    "        float(TUAG)                                         # 14 TUAG\n",
    "    ]\n",
    "\n",
    "    classification[i] = 1 if \"Pt\" in pid else 0\n",
    "\n",
    "feature_names = np.array([\n",
    "    \"Stride Time Var.\",        \"Swing Time Var.\",    \"Step Time Var.\",\n",
    "    \"Stride Time Asym.\",       \"Swing Time Asym.\",   \"Step Time Asym.\",\n",
    "    \"Stance Time Asym.\",       \"Double Support\",     \"Heel-Toe Pressure\",\n",
    "    \"Heel-Toe Pressure Asym.\", \"Gender\",             \"Age\",\n",
    "    \"Speed\",                   \"Height\",             \"TUAG\"\n",
    "])\n",
    "\n",
    "print_feature_stats(features, feature_names)\n"
   ],
   "id": "6968e4a867cb260a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Summary Statistics ===\n",
      "Feature & Min & Max & Mean & Std & Median \\\\\n",
      "---------------------------------------------\n",
      "Stride Time Var. & 0.03 & 0.55 & 0.13 & 0.08 & 0.12\\\\\n",
      "Swing Time Var. & 0.09 & 0.64 & 0.28 & 0.12 & 0.25\\\\\n",
      "Step Time Var. & 0.04 & 0.77 & 0.22 & 0.15 & 0.17\\\\\n",
      "Stride Time Asym. & 0.86 & 1.17 & 1.00 & 0.03 & 1.00\\\\\n",
      "Swing Time Asym. & 0.68 & 1.37 & 0.99 & 0.12 & 0.99\\\\\n",
      "Step Time Asym. & 0.36 & 3.48 & 1.08 & 0.41 & 1.02\\\\\n",
      "Stance Time Asym. & 0.89 & 1.12 & 1.00 & 0.04 & 1.00\\\\\n",
      "Double Support & 0.21 & 0.55 & 0.32 & 0.06 & 0.30\\\\\n",
      "Heel-Toe Pressure & 0.47 & 2.20 & 1.05 & 0.36 & 1.00\\\\\n",
      "Heel-Toe Pressure Asym. & 0.24 & 2.85 & 1.11 & 0.44 & 1.10\\\\\n",
      "Gender & 0.00 & 1.00 & 0.59 & 0.50 & 1.00\\\\\n",
      "Age & 36.00 & 86.00 & 64.53 & 9.88 & 65.00\\\\\n",
      "Speed & 0.36 & 1.54 & 1.14 & 0.23 & 1.15\\\\\n",
      "Height & 1.45 & 1.95 & 1.68 & 0.09 & 1.68\\\\\n",
      "TUAG & 6.23 & 36.34 & 11.06 & 3.89 & 10.46\\\\\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=Feature Re-Scaling=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "bbff2edb11ce24e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T10:52:45.892927Z",
     "start_time": "2025-10-25T10:52:45.889562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Applies Re-scaling to Features\n",
    "features = MinMaxScaler().fit_transform(features)\n",
    "print_feature_stats(features, feature_names)"
   ],
   "id": "427d8123c947e7d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Summary Statistics ===\n",
      "Feature & Min & Max & Mean & Std & Median \\\\\n",
      "---------------------------------------------\n",
      "Stride Time Var. & 0.00 & 1.00 & 0.19 & 0.16 & 0.18\\\\\n",
      "Swing Time Var. & 0.00 & 1.00 & 0.34 & 0.21 & 0.29\\\\\n",
      "Step Time Var. & 0.00 & 1.00 & 0.24 & 0.21 & 0.17\\\\\n",
      "Stride Time Asym. & 0.00 & 1.00 & 0.45 & 0.11 & 0.45\\\\\n",
      "Swing Time Asym. & 0.00 & 1.00 & 0.46 & 0.18 & 0.45\\\\\n",
      "Step Time Asym. & 0.00 & 1.00 & 0.23 & 0.13 & 0.21\\\\\n",
      "Stance Time Asym. & 0.00 & 1.00 & 0.49 & 0.18 & 0.49\\\\\n",
      "Double Support & 0.00 & 1.00 & 0.31 & 0.18 & 0.27\\\\\n",
      "Heel-Toe Pressure & 0.00 & 1.00 & 0.34 & 0.21 & 0.31\\\\\n",
      "Heel-Toe Pressure Asym. & 0.00 & 1.00 & 0.33 & 0.17 & 0.33\\\\\n",
      "Gender & 0.00 & 1.00 & 0.59 & 0.50 & 1.00\\\\\n",
      "Age & 0.00 & 1.00 & 0.57 & 0.20 & 0.58\\\\\n",
      "Speed & 0.00 & 1.00 & 0.66 & 0.19 & 0.67\\\\\n",
      "Height & 0.00 & 1.00 & 0.45 & 0.18 & 0.46\\\\\n",
      "TUAG & 0.00 & 1.00 & 0.16 & 0.13 & 0.14\\\\\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=Feature Re-Scaling=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "60f30f8a2313c707"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T11:14:02.013914Z",
     "start_time": "2025-10-25T11:14:02.009416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "My KNN function- for reproducability between code and report, I use my student number as the random state. I do cross validation both for KNN and SFS, and\n",
    "so I use my student number +1 for the second cross-validation to ensure randomness. I use ROC-auc as the scoring metric, as that is the metric I care most about.\n",
    "I use stratified Kfolds to ensure some level of balance between folds, due to the small dataset and high variance.\n",
    "\"\"\"\n",
    "\n",
    "def knn(X, y, K, weights=\"uniform\"):\n",
    "    #Initilizes Folds folds for KNN and SFS\n",
    "    KNN_folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=21207103)\n",
    "    SFS_folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=21207103+1)\n",
    "    base_knn = KNeighborsClassifier(n_neighbors=K, weights=weights)\n",
    "    metrics, fold_details = [], []\n",
    "\n",
    "    #Loop representing each KNN Fold\n",
    "    for tr, te in KNN_folds.split(X, y):\n",
    "        #Initilize SFS, ensuring that estimator is the same as what will model (as a clone to avoid leakage\n",
    "        sfs = SequentialFeatureSelector(\n",
    "            estimator=clone(base_knn),\n",
    "            n_features_to_select=\"auto\",\n",
    "            direction=\"forward\",\n",
    "            scoring=\"roc_auc\",\n",
    "            tol=1e-5,\n",
    "            cv=SFS_folds,\n",
    "            n_jobs=-1\n",
    "            )\n",
    "        #applying sfs and fitting\n",
    "        pipe = Pipeline([\n",
    "            (\"sfs\", clone(sfs)),\n",
    "            (\"knn\", clone(base_knn))\n",
    "        ])\n",
    "        pipe.fit(X[tr], y[tr])\n",
    "\n",
    "        #testing and getting results\n",
    "        yp = pipe.predict(X[te])\n",
    "        acc = accuracy_score(y[te], yp)\n",
    "        f1m = f1_score(y[te], yp, average=\"macro\")\n",
    "        auc = roc_auc_score(y[te], pipe.predict_proba(X[te])[:, 1])\n",
    "        n_selected = np.sum(pipe.named_steps[\"sfs\"].get_support())\n",
    "\n",
    "        #appending rsults for return\n",
    "        fold_details.append([acc, f1m, auc, n_selected])\n",
    "        metrics.append([acc, f1m, auc])\n",
    "\n",
    "    return np.array(metrics), fold_details\n",
    "\n",
    "#Printing functions for visibility\n",
    "def print_knn(scores, K):\n",
    "    mean, std = np.nanmean(scores, axis=0), np.nanstd(scores, axis=0)\n",
    "    print(\n",
    "        f\"{K} & \"\n",
    "        f\"{mean[0]:.2f} & {std[0]:.2f} & \"\n",
    "        f\"{mean[1]:.2f} & {std[1]:.2f} & \"\n",
    "        f\"{mean[2]:.2f} & {std[2]:.2f}\\\\\\\\\"\n",
    "    )\n",
    "\n",
    "#Printing functions for visibility\n",
    "def print_knn_folds(fold_details, K, weights):\n",
    "    print(f\"Per-fold results (K={K}, weights={weights}):\")\n",
    "    print(\"Fold & Acc & F1 & AUC & #Sel\\\\\\\\\")\n",
    "    for acc, f1m, auc, nsel in fold_details:\n",
    "        print(f\"{acc:.2f} & {f1m:.2f} & {auc:.2f} & {int(nsel)}\\\\\\\\\")\n"
   ],
   "id": "1002944bcbfec75",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T12:23:29.020321Z",
     "start_time": "2025-10-25T12:22:25.939149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "print(\"K  & Mean Acc. & Acc. Std. Dev. & Mean F1 & F1 Std. Dev. & Mean AUC & AUC Std. Dev\")\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    scores, folds = knn(features, classification, K=K, weights=\"uniform\")\n",
    "    print_knn(scores, K)     # table line\n",
    "    results[K] = (scores, folds)\n",
    "\n",
    "# --- after summary table ---\n",
    "print(\"\\nDetailed per-fold results:\")\n",
    "for K, (scores, folds) in results.items():\n",
    "    print_knn_folds(folds, K, \"uniform\")\n",
    "    print()  # blank line between K-values"
   ],
   "id": "26aa0782382d9103",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K  & Mean Acc. & Acc. Std. Dev. & Mean F1 & F1 Std. Dev. & Mean AUC & AUC Std. Dev\n",
      "1 & 0.64 & 0.10 & 0.63 & 0.10 & 0.64 & 0.10\\\\\n",
      "3 & 0.62 & 0.10 & 0.61 & 0.10 & 0.65 & 0.12\\\\\n",
      "5 & 0.61 & 0.14 & 0.60 & 0.14 & 0.72 & 0.11\\\\\n",
      "7 & 0.71 & 0.19 & 0.70 & 0.19 & 0.81 & 0.16\\\\\n",
      "9 & 0.61 & 0.17 & 0.59 & 0.19 & 0.73 & 0.16\\\\\n",
      "\n",
      "Detailed per-fold results:\n",
      "Per-fold results (K=1, weights=uniform):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 5\\\\\n",
      "79 & 0.88 & 0.87 & 0.88 & 6\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 6\\\\\n",
      "79 & 0.50 & 0.50 & 0.50 & 3\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 2\\\\\n",
      "79 & 0.75 & 0.73 & 0.75 & 6\\\\\n",
      "79 & 0.50 & 0.50 & 0.50 & 3\\\\\n",
      "\n",
      "Per-fold results (K=3, weights=uniform):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.75 & 0.73 & 0.75 & 4\\\\\n",
      "79 & 0.75 & 0.73 & 0.62 & 2\\\\\n",
      "79 & 0.75 & 0.73 & 0.75 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.59 & 2\\\\\n",
      "79 & 0.62 & 0.62 & 0.53 & 2\\\\\n",
      "79 & 0.62 & 0.62 & 0.66 & 7\\\\\n",
      "79 & 0.50 & 0.50 & 0.44 & 2\\\\\n",
      "79 & 0.50 & 0.47 & 0.72 & 2\\\\\n",
      "79 & 0.62 & 0.62 & 0.88 & 4\\\\\n",
      "79 & 0.50 & 0.50 & 0.56 & 4\\\\\n",
      "\n",
      "Per-fold results (K=5, weights=uniform):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.75 & 0.75 & 0.84 & 3\\\\\n",
      "79 & 0.62 & 0.62 & 0.75 & 3\\\\\n",
      "79 & 0.75 & 0.73 & 0.84 & 3\\\\\n",
      "79 & 0.75 & 0.75 & 0.78 & 3\\\\\n",
      "79 & 0.62 & 0.62 & 0.59 & 2\\\\\n",
      "79 & 0.38 & 0.37 & 0.62 & 5\\\\\n",
      "79 & 0.50 & 0.50 & 0.72 & 2\\\\\n",
      "79 & 0.62 & 0.56 & 0.81 & 4\\\\\n",
      "79 & 0.75 & 0.73 & 0.75 & 4\\\\\n",
      "79 & 0.38 & 0.37 & 0.50 & 5\\\\\n",
      "\n",
      "Per-fold results (K=7, weights=uniform):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.75 & 0.73 & 1.00 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.88 & 3\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.75 & 0.75 & 0.75 & 3\\\\\n",
      "79 & 0.50 & 0.47 & 0.59 & 1\\\\\n",
      "79 & 0.75 & 0.73 & 0.78 & 5\\\\\n",
      "79 & 1.00 & 1.00 & 1.00 & 3\\\\\n",
      "79 & 0.75 & 0.75 & 0.88 & 3\\\\\n",
      "79 & 1.00 & 1.00 & 1.00 & 3\\\\\n",
      "79 & 0.38 & 0.37 & 0.59 & 4\\\\\n",
      "\n",
      "Per-fold results (K=9, weights=uniform):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.62 & 0.62 & 0.66 & 2\\\\\n",
      "79 & 0.75 & 0.73 & 0.88 & 6\\\\\n",
      "79 & 0.75 & 0.75 & 0.91 & 4\\\\\n",
      "79 & 0.50 & 0.47 & 0.56 & 3\\\\\n",
      "79 & 0.50 & 0.47 & 0.69 & 2\\\\\n",
      "79 & 0.62 & 0.56 & 0.75 & 4\\\\\n",
      "79 & 0.50 & 0.47 & 0.53 & 1\\\\\n",
      "79 & 0.75 & 0.73 & 0.94 & 5\\\\\n",
      "79 & 0.88 & 0.87 & 0.91 & 2\\\\\n",
      "79 & 0.25 & 0.20 & 0.47 & 5\\\\\n",
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T12:24:14.051939Z",
     "start_time": "2025-10-25T12:23:29.022159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "print(\"K  & Mean Acc. & Acc. Std. Dev. & Mean F1 & F1 Std. Dev. & Mean AUC & AUC Std. Dev\")\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    scores, folds = knn(features, classification, K=K, weights=\"distance\")\n",
    "    print_knn(scores, K)     # table line\n",
    "    results[K] = (scores, folds)\n",
    "\n",
    "# --- after summary table ---\n",
    "print(\"\\nDetailed per-fold results:\")\n",
    "for K, (scores, folds) in results.items():\n",
    "    print_knn_folds(folds, K, \"distance\")\n",
    "    print()  # blank line between K-values"
   ],
   "id": "a2ff9784a90b4705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K  & Mean Acc. & Acc. Std. Dev. & Mean F1 & F1 Std. Dev. & Mean AUC & AUC Std. Dev\n",
      "1 & 0.64 & 0.10 & 0.63 & 0.10 & 0.64 & 0.10\\\\\n",
      "3 & 0.70 & 0.15 & 0.69 & 0.16 & 0.71 & 0.17\\\\\n",
      "5 & 0.70 & 0.15 & 0.69 & 0.16 & 0.78 & 0.16\\\\\n",
      "7 & 0.57 & 0.18 & 0.56 & 0.19 & 0.68 & 0.20\\\\\n",
      "9 & 0.64 & 0.21 & 0.63 & 0.22 & 0.68 & 0.23\\\\\n",
      "\n",
      "Detailed per-fold results:\n",
      "Per-fold results (K=1, weights=distance):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 5\\\\\n",
      "79 & 0.88 & 0.87 & 0.88 & 6\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 4\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 6\\\\\n",
      "79 & 0.50 & 0.50 & 0.50 & 3\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 2\\\\\n",
      "79 & 0.75 & 0.73 & 0.75 & 6\\\\\n",
      "79 & 0.50 & 0.50 & 0.50 & 3\\\\\n",
      "\n",
      "Per-fold results (K=3, weights=distance):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.75 & 0.73 & 0.62 & 4\\\\\n",
      "79 & 0.75 & 0.73 & 0.81 & 7\\\\\n",
      "79 & 0.62 & 0.56 & 0.62 & 6\\\\\n",
      "79 & 0.88 & 0.87 & 0.88 & 5\\\\\n",
      "79 & 0.75 & 0.75 & 0.75 & 2\\\\\n",
      "79 & 0.88 & 0.87 & 0.81 & 5\\\\\n",
      "79 & 0.50 & 0.50 & 0.41 & 2\\\\\n",
      "79 & 0.50 & 0.47 & 0.69 & 2\\\\\n",
      "79 & 0.88 & 0.87 & 1.00 & 4\\\\\n",
      "79 & 0.50 & 0.50 & 0.50 & 5\\\\\n",
      "\n",
      "Per-fold results (K=5, weights=distance):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.62 & 0.56 & 0.62 & 6\\\\\n",
      "79 & 0.75 & 0.73 & 0.62 & 2\\\\\n",
      "79 & 0.75 & 0.73 & 0.88 & 3\\\\\n",
      "79 & 0.75 & 0.75 & 0.88 & 5\\\\\n",
      "79 & 0.88 & 0.87 & 0.94 & 3\\\\\n",
      "79 & 0.88 & 0.87 & 0.94 & 3\\\\\n",
      "79 & 0.50 & 0.50 & 0.69 & 2\\\\\n",
      "79 & 0.50 & 0.47 & 0.62 & 2\\\\\n",
      "79 & 0.88 & 0.87 & 1.00 & 5\\\\\n",
      "79 & 0.50 & 0.50 & 0.56 & 4\\\\\n",
      "\n",
      "Per-fold results (K=7, weights=distance):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.50 & 0.50 & 0.62 & 3\\\\\n",
      "79 & 0.62 & 0.56 & 0.69 & 5\\\\\n",
      "79 & 0.75 & 0.75 & 0.81 & 4\\\\\n",
      "79 & 0.25 & 0.20 & 0.19 & 1\\\\\n",
      "79 & 0.88 & 0.87 & 1.00 & 3\\\\\n",
      "79 & 0.75 & 0.73 & 0.81 & 5\\\\\n",
      "79 & 0.50 & 0.50 & 0.75 & 2\\\\\n",
      "79 & 0.50 & 0.47 & 0.62 & 2\\\\\n",
      "79 & 0.62 & 0.62 & 0.75 & 4\\\\\n",
      "79 & 0.38 & 0.37 & 0.56 & 6\\\\\n",
      "\n",
      "Per-fold results (K=9, weights=distance):\n",
      "Fold & Acc & F1 & AUC & #Sel\\\\\n",
      "79 & 0.50 & 0.47 & 0.38 & 4\\\\\n",
      "79 & 0.88 & 0.87 & 1.00 & 3\\\\\n",
      "79 & 0.88 & 0.87 & 0.75 & 4\\\\\n",
      "79 & 0.25 & 0.20 & 0.25 & 1\\\\\n",
      "79 & 0.75 & 0.75 & 0.69 & 2\\\\\n",
      "79 & 0.88 & 0.87 & 0.88 & 3\\\\\n",
      "79 & 0.50 & 0.50 & 0.69 & 2\\\\\n",
      "79 & 0.62 & 0.62 & 0.62 & 3\\\\\n",
      "79 & 0.75 & 0.73 & 0.94 & 5\\\\\n",
      "79 & 0.38 & 0.37 & 0.56 & 4\\\\\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=Feature Re-Scaling=-=-=-=-=-=-=-=-=-=-=-=-=-=-",
   "id": "5872c04e5116d4a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T11:34:18.443923Z",
     "start_time": "2025-10-25T11:34:18.439374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "My SVM function- again, I use my student number as the random state for reproducabiltiy between the two different folds. I again do cross validation both for SVM and SFS. The structure is very similar, except that it uses a SVM rather then KNN. I also have an input to select the value for C and the number of features. I do not use a kernel, as upon testing it did not provide any performance increase, but significantly affected speed\n",
    "\"\"\"\n",
    "\n",
    "def svm(X, y, C_value, n_features=None):\n",
    "    #Initlizing folds\n",
    "    SVMFolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=21207103)\n",
    "    SFSFolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=21207103+1)\n",
    "\n",
    "    #Initlize SVM and SFS\n",
    "    base_svm = SVC(C=C_value)\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=clone(base_svm),\n",
    "        n_features_to_select=\"auto\" if n_features is None else n_features,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"roc_auc\",\n",
    "        tol=1e-5,\n",
    "        cv=SFSFolds,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    metrics = []\n",
    "    n_selected = []\n",
    "\n",
    "    #Represent Each SVM fold\n",
    "    for tr, te in SVMFolds.split(X, y):\n",
    "        #Pipeline and fitting\n",
    "        pipe = Pipeline([\n",
    "            (\"sfs\", clone(sfs)),\n",
    "            (\"svm\", clone(base_svm))\n",
    "        ])\n",
    "        pipe.fit(X[tr], y[tr])\n",
    "        n_selected.append(len(pipe.named_steps[\"sfs\"].get_support(indices=True)))\n",
    "\n",
    "        #metric Calculation\n",
    "        yp = pipe.predict(X[te])\n",
    "        scores = pipe.decision_function(X[te])\n",
    "        acc = accuracy_score(y[te], yp)\n",
    "        f1 = f1_score(y[te], yp, average=\"binary\")\n",
    "        auc = roc_auc_score(y[te], scores)\n",
    "        metrics.append([acc, f1, auc])\n",
    "\n",
    "    return np.array(metrics), np.array(n_selected)\n",
    "\n",
    "\n",
    "#Function to print info in latex form\n",
    "def print_svm(scores, C, subset_sizes):\n",
    "    mean, std = np.nanmean(scores, axis=0), np.nanstd(scores, axis=0)\n",
    "    mean_subset = np.mean(subset_sizes)\n",
    "\n",
    "    print(\n",
    "        f\"{C} & \"\n",
    "        f\"{mean_subset:.2f} & \"\n",
    "        f\"{mean[0]:.2f} & {std[0]:.2f} & \"\n",
    "        f\"{mean[1]:.2f} & {std[1]:.2f} & \"\n",
    "        f\"{mean[2]:.2f} & {std[2]:.2f}\\\\\\\\\"\n",
    "    )\n"
   ],
   "id": "adf6500fd8876774",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= SVM AUTO =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=",
   "id": "66f98451868b7e30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T12:26:20.942315Z",
     "start_time": "2025-10-25T12:25:39.169460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "print(\"SVM Auto\")\n",
    "print(\"C  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\\\\\")\n",
    "for C in [0.1, .5, 1, 5, 10]:\n",
    "\n",
    "    metrics_auto, subset_sizes = svm(features, classification, C)\n",
    "    print_svm(metrics_auto, C, subset_sizes)\n",
    "    results.append(subset_sizes)\n",
    "\n",
    "print(\"\\nPer Fold Feature Selection Sizing\")\n",
    "print(results)\n"
   ],
   "id": "60c9ee7ecc2477b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Auto\n",
      "C  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\n",
      "0.1 & 3.50 & 0.70 & 0.18 & 0.61 & 0.28 & 0.82 & 0.16\\\\\n",
      "0.5 & 3.50 & 0.71 & 0.14 & 0.65 & 0.19 & 0.80 & 0.11\\\\\n",
      "1 & 4.00 & 0.64 & 0.13 & 0.62 & 0.14 & 0.78 & 0.12\\\\\n",
      "5 & 3.00 & 0.74 & 0.12 & 0.74 & 0.11 & 0.82 & 0.10\\\\\n",
      "10 & 3.80 & 0.75 & 0.14 & 0.72 & 0.18 & 0.85 & 0.12\\\\\n",
      "\n",
      "Per Fold Feature Selection Sizing\n",
      "[array([3, 3, 3, 2, 3, 3, 4, 4, 5, 5]), array([4, 3, 3, 2, 3, 3, 4, 4, 4, 5]), array([4, 4, 4, 3, 2, 4, 4, 5, 5, 5]), array([3, 3, 3, 3, 2, 4, 4, 3, 2, 3]), array([3, 4, 6, 3, 3, 6, 4, 3, 3, 3])]\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= SVM Fixed =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=",
   "id": "722e280804d26377"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T12:56:00.486753Z",
     "start_time": "2025-10-25T12:54:50.986025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "print(\"SVM Fixed\")\n",
    "print(\"C  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\\\\\")\n",
    "for C in [0.1, .5, 1, 5, 10]:\n",
    "    metrics_fixed, subset_sizes = svm(features, classification, C, n_features=np.max(subset_sizes))\n",
    "    print_svm(metrics_fixed, C, subset_sizes)\n",
    "    results.append(subset_sizes)\n",
    "\n",
    "print(\"Per Fold Feature Selection Sizing\")\n",
    "print(results)"
   ],
   "id": "b26f72725de9fc2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Fixed\n",
      "C  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\n",
      "0.1 & 6.00 & 0.66 & 0.13 & 0.62 & 0.13 & 0.81 & 0.15\\\\\n",
      "0.5 & 6.00 & 0.64 & 0.13 & 0.61 & 0.15 & 0.77 & 0.15\\\\\n",
      "1 & 6.00 & 0.70 & 0.15 & 0.69 & 0.15 & 0.81 & 0.16\\\\\n",
      "5 & 6.00 & 0.74 & 0.14 & 0.74 & 0.15 & 0.89 & 0.10\\\\\n",
      "10 & 6.00 & 0.71 & 0.08 & 0.65 & 0.13 & 0.85 & 0.14\\\\\n",
      "Per Fold Feature Selection Sizing\n",
      "[array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])]\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Random Forest =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=",
   "id": "93f6df72dd9c79d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T14:50:20.492868Z",
     "start_time": "2025-10-25T14:50:20.488698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "The final model, which is again quite similar to KNN and SVM. However, this takes a considerable amount of time to run, so I have had to remove the cross validation from the SFS out of time requirements. I still use the fixed seed of my student number, and otherwise the function is structured quite simialr\n",
    "\"\"\"\n",
    "\n",
    "def rf(X, y, n_trees, n_features=None):\n",
    "    #fold initilization\n",
    "    Folds = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    metrics, subset_sizes = [], []\n",
    "\n",
    "    #rf and sfs initlized\n",
    "    n_features = \"auto\" if n_features is None else n_features\n",
    "    base_rf = RandomForestClassifier(n_estimators=n_trees, max_depth=10)\n",
    "    sfs = SequentialFeatureSelector(estimator=clone(base_rf), n_features_to_select=n_features, tol=1e-5)\n",
    "\n",
    "    #loop representing each fold of rf\n",
    "    for tr, te in Folds.split(X, y):\n",
    "\n",
    "        #model sfsing and fitting\n",
    "        pipe = Pipeline([\n",
    "            (\"sfs\", clone(sfs)),\n",
    "            (\"rf\", clone(base_rf))\n",
    "        ])\n",
    "        pipe.fit(X[tr], y[tr])\n",
    "        yp = pipe.predict(X[te])\n",
    "        proba = pipe.predict_proba(X[te])[:, 1]\n",
    "\n",
    "        #metric calcualtion and appending\n",
    "        acc = accuracy_score(y[te], yp)\n",
    "        f1m = f1_score(y[te], yp, average=\"macro\")\n",
    "        auc = roc_auc_score(y[te], proba)\n",
    "        ksel = pipe.named_steps[\"sfs\"].get_support(indices=True).size\n",
    "        subset_sizes.append(ksel)\n",
    "        metrics.append([acc, f1m, auc])\n",
    "\n",
    "    return np.array(metrics), np.array(subset_sizes)\n",
    "\n",
    "\n",
    "\n",
    "#function to print data in neat way\n",
    "def print_rf(scores, n_trees, subset_sizes):\n",
    "    mean, std = np.nanmean(scores, axis=0), np.nanstd(scores, axis=0)\n",
    "    mean_subset = np.mean(subset_sizes)\n",
    "    row = (\n",
    "        f\"{n_trees} & \"\n",
    "        f\"{mean_subset:.2f} & \"\n",
    "        f\"{mean[0]:.2f} & {std[0]:.2f} & \"\n",
    "        f\"{mean[1]:.2f} & {std[1]:.2f} & \"\n",
    "        f\"{mean[2]:.2f} & {std[2]:.2f}\\\\\\\\\"\n",
    "    )\n",
    "    print(row)\n",
    "\n"
   ],
   "id": "b7e1e68ff7e5c64a",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Random Forest With Auto Feature Selection=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=",
   "id": "80a861aff30c90f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T14:53:27.047810Z",
     "start_time": "2025-10-25T14:50:25.785295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Random Forest\")\n",
    "print(\"Trees  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\\\\\")\n",
    "results = []\n",
    "for n_trees in [20, 100]:\n",
    "    # --- 1) AUTO run ---\n",
    "    scores_auto, sizes_auto = rf(features, classification, n_trees, n_features=None)  # None -> \"auto\" inside your fn\n",
    "    print_rf(scores_auto, n_trees, sizes_auto)\n",
    "    results.append(sizes_auto)\n",
    "\n",
    "print(results)\n",
    "\n"
   ],
   "id": "f19d7fc5e0b2f0f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Trees  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\n",
      "20 & 3.50 & 0.62 & 0.12 & 0.61 & 0.13 & 0.71 & 0.15\\\\\n",
      "100 & 3.40 & 0.69 & 0.10 & 0.68 & 0.11 & 0.75 & 0.15\\\\\n",
      "[array([4, 3, 4, 3, 3, 3, 4, 3, 4, 4]), array([2, 3, 5, 2, 3, 4, 3, 4, 3, 5])]\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= Random Forest With Fixed Feature Count =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=",
   "id": "6734609369739aaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:26:51.986068Z",
     "start_time": "2025-10-25T15:20:30.681636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Random Forest\")\n",
    "print(\"Trees  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\\\\\")\n",
    "results1 = []\n",
    "for n_trees in [20, 100]:\n",
    "    scores_kplus, sizes_kplus = rf(features, classification, n_trees, n_features=np.max(results)+1)\n",
    "    print_rf(scores_kplus, n_trees, sizes_kplus)\n",
    "    results1.append(sizes_kplus)\n",
    "\n",
    "print(results1)"
   ],
   "id": "7c7b3bde2ae89d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Trees  & Mean Feature Ct. & Mean Acc. & Acc. SD & Mean F1 & F1 SD & Mean AUC & AUC SD\\\\\n",
      "20 & 6.00 & 0.69 & 0.10 & 0.67 & 0.11 & 0.71 & 0.16\\\\\n",
      "100 & 6.00 & 0.70 & 0.16 & 0.68 & 0.17 & 0.78 & 0.16\\\\\n",
      "[array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])]\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
